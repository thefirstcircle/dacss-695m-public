{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After regex pre-processing names, find artists whose art is sold >10 times in the data set:\n",
    "\n",
    "artists_more_than_10_unique = pd.DataFrame(artists_more_than_10['name'].unique(), columns=['name'])\n",
    "\n",
    "# Optionally, remove values like 'french school, 19th century':\n",
    "\n",
    "artists_more_than_10_unique = artists_more_than_10_unique[~artists_more_than_10_unique['name'].str.contains('school')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some weird whitespaced characters in the data, so clean as necessary:\n",
    "\n",
    "artists_more_than_10_unique = artists_more_than_10_unique.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "artists_more_than_10_unique = artists_more_than_10_unique.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Output to CSV:\n",
    "\n",
    "artists_more_than_10_unique.to_csv('artists_more_than_10_unique.csv', float_format='%.2f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_wiki(df, csv_path):\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['name', 'artist_wiki_references', 'artist_wiki_links', 'artist_categories', 'artist_most_common_nationality']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for artist in df['name'].unique():\n",
    "            try:\n",
    "                # Search for the artist's name followed by the keyword\n",
    "                search_query = artist\n",
    "                search_results = wikipedia.search(search_query)\n",
    "\n",
    "                # If no results found, try again with \"artist\" appended\n",
    "                if len(search_results) == 0:\n",
    "                    search_query = artist + \" (artist)\"\n",
    "                    search_results = wikipedia.search(search_query)\n",
    "\n",
    "                # If no results found, try again with \"painter\" appended\n",
    "                if len(search_results) == 0:\n",
    "                    search_query = artist + \" (painter)\"\n",
    "                    search_results = wikipedia.search(search_query)\n",
    "\n",
    "              
    "                if len(search_results) == 0:\n",
    "                    print(f\"Could not find page for {artist}. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "                # Get the page object for the first search result, then search inside their summary for keywords\n",
    "                page = wikipedia.page(search_results[0], auto_suggest = False)\n",
    "                if any(keyword in page.summary.lower() for keyword in [\"artist\", \"painter\", \"sculptor\"]):\n",
    "                    print(f\"Found keywords for artist, painter, or sculptor on the Wikipedia page for {artist}.\")\n",
    "                    \n",
    "                    # Get the number of references and links\n",
    "                    if hasattr(page, 'references'):\n",
    "                        num_references = len(page.references)\n",
    "                    else:\n",
    "                        num_references = 0\n",
    "\n",
    "                    if hasattr(page, 'links'):\n",
    "                        num_links = len(page.links)\n",
    "                    else:\n",
    "                        num_links = 0 \n",
    "                    \n",
    "                    # Get the categories that the page belongs to\n",
    "                    categories = page.categories\n",
    "\n",
    "                    # Using spaCy NLP, find most-often occuring nationality in page.categories\n",
    "                    # Extract the nationalities from the categories and count them\n",
    "                    nationalities = []\n",
    "                    for category in categories:\n",
    "                        doc = nlp(category)\n",
    "                        for ent in doc.ents:\n",
    "                            if ent.label_ == \"NORP\":\n",
    "                                nationalities.append(ent.text)\n",
    "                    if len(nationalities) > 0:\n",
    "                        artist_most_common_nationality = Counter(nationalities).most_common(1)[0][0]\n",
    "                    else:\n",
    "                        artist_most_common_nationality = None\n",
    "\n",
    "                    # Write a new row to the CSV file\n",
    "                    writer.writerow({'name': artist, 'artist_wiki_references': num_references, \n",
    "                                      'artist_wiki_links': num_links, 'artist_categories': \", \".join(categories), 'artist_most_common_nationality': artist_most_common_nationality})\n",
    "                    print(f\"{artist} has {num_references} references and {num_links} links.\")\n",
    "\n",
    "            except (PageError, DisambiguationError, KeyError):\n",
    "                    print(f\"Error: Could not find page for {artist}. Skipping...\")\n",
    "                    continue\n",
    "            \n",
    "            # Avoid Wikimedia rate-limiting\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "\n",
    "get_artist_wiki(artists_more_than_10_unique, csv_path=\"artists_wiki_refs_nationalities.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
